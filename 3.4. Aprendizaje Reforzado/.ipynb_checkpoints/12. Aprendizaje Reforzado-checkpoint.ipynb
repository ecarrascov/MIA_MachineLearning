{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xZJ_-mef54i",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Aprendizaje Reforzado\n",
    "\n",
    "**Nota**: Este _notebook_ está basado en el trabajo hecho por el profesor Gonzalo Ruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBIrDNnPWEch"
   },
   "source": [
    "La mayoría de ustedes probablemente han oído hablar de que la inteligencia artificial aprende a jugar juegos de computadora por su cuenta, un ejemplo muy popular es el caso de **Deepmind**. Deepmind llegó a las noticias cuando su programa AlphaGo derrotó al campeón mundial de Go de Corea del Sur en 2016. Hubo muchos intentos exitosos en el pasado para desarrollar agentes con la intención de jugar juegos de Atari como Breakout, Pong y Space Invaders.\n",
    "\n",
    "Cada uno de estos programas sigue un paradigma de aprendizaje automático conocido como __aprendizaje por refuerzo__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sy1U2AsAWmZH"
   },
   "source": [
    "## ¿Qué es el aprendizaje reforzado?\n",
    "\n",
    "Considera el escenario de enseñarle nuevos trucos a un perro. El perro no entiende nuestro idioma, por lo que no podemos decirle qué hacer. Por eso, en general seguimos una estrategia diferente. Emulamos una situación (o una señal) y el perro intenta responder de muchas formas diferentes. Si la respuesta del perro es la deseada, lo premiamos con bocadillos. Entonces, la próxima vez que el perro se exponga a la misma situación, ejecutará una acción similar con aún más entusiasmo a la espera de más comida. Eso es como aprender \"qué hacer\" a partir de experiencias positivas. Del mismo modo, los perros tenderán a aprender qué no hacer cuando se enfrenten a experiencias negativas.\n",
    "\n",
    "Así es exactamente como funciona el aprendizaje por refuerzo en un sentido más amplio: \n",
    "\n",
    "* El perro es un \"agente\" que está expuesto al **medio ambiente** o **el entorno**. El medio ambiente podría estar en tu casa, contigo.\n",
    "* Las situaciones que encuentran son análogas a un **estado**. Un ejemplo de un estado podría ser tu perro parado y usas una palabra específica en un tono determinado en tu sala de estar\n",
    "* Nuestros agentes reaccionan realizando una **acción** para pasar de un \"estado\" a otro \"estado\", por ejemplo, su perro pasa de estar de pie a sentado.\n",
    "* Después de la transición, pueden recibir una **recompensa** o una **penalización** a cambio. ¡Les das un regalo! O un \"No\" como penalización.\n",
    "* La **política** es la estrategia de elegir una acción en un estado con la expectativa de mejores resultados.\n",
    "\n",
    "El aprendizaje por refuerzo se encuentra entre el espectro del aprendizaje supervisado y el aprendizaje no supervisado, y hay algunas cosas importantes a tener en cuenta:\n",
    "\n",
    "1.   **Ser avaro (greedy) no siempre funciona**\n",
    "\n",
    ">Hay cosas que son fáciles de hacer para obtener una gratificación instantánea, y hay cosas que brindan recompensas a largo plazo. El objetivo es no ser _greedy_ buscando recompensas inmediatas rápidas, sino optimizar para obtener las máximas recompensas durante todo el entrenamiento.\n",
    "\n",
    "2.   **La secuencia o el orden importa en RL**\n",
    "\n",
    ">El agente de recompensa no solo depende del estado actual, sino de toda la historia de los estados. A diferencia del aprendizaje supervisado y no supervisado, el tiempo es importante aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ablbqk5aDbI"
   },
   "source": [
    "## El Proceso de RL\n",
    "\n",
    "En cierto modo, el aprendizaje por refuerzo es la ciencia de tomar decisiones óptimas utilizando experiencias. Al desglosarlo, el proceso de aprendizaje por refuerzo incluye estos sencillos pasos:\n",
    "\n",
    "1. Observación del medio ambiente (o el entorno)\n",
    "2. Decidir cómo actuar usando alguna estrategia\n",
    "3. Actuando en consecuencia\n",
    "4. Recibir una recompensa o una penalización\n",
    "5. Aprender de las experiencias y perfeccionar nuestra estrategia\n",
    "6. Iterar hasta encontrar una estrategia óptima\n",
    "\n",
    "Veamos un ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrHJF9u-bWkx"
   },
   "source": [
    "## Ejemplo: Taxi Autónomo\n",
    "\n",
    "Diseñemos una simulación de un taxi autónomo. El objetivo principal es demostrar, en un entorno simplificado, cómo se puede utilizar las técnicas de RL para desarrollar un enfoque eficiente y seguro para abordar este problema.\n",
    "\n",
    "El trabajo del taxi inteligente es recoger al pasajero en un lugar y dejarlo en otro.\n",
    "\n",
    "El taxi inteligente debe cumplir con:\n",
    "\n",
    "* Dejar al pasajero en el lugar correcto.\n",
    "* Ahorrar tiempo al pasajero tomando el mínimo tiempo posible para dejar al pasajero(a) en su destino.\n",
    "* Cuidar las normas de tráfico y la seguridad de los pasajeros.\n",
    "* Hay diferentes aspectos que deben considerarse aquí al modelar una solución de RL para este problema: *recompensas*, *estados* y *acciones*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0BYbCKiiefd"
   },
   "source": [
    "### 1. Recompensas\n",
    "Dado que el agente (el conductor imaginario) está motivado por las recompensas y va a aprender a controlar el taxi mediante experiencias de prueba en el entorno, debemos decidir las **recompensas** y / o **sanciones** y su magnitud en consecuencia. Aquí algunos puntos a considerar:\n",
    "\n",
    "* El agente debe recibir una alta recompensa positiva si logra dejar al pasajero en el lugar correcto porque este comportamiento es muy deseado\n",
    "\n",
    "* El agente debe ser penalizado si intenta dejar a un pasajero en lugares incorrectos.\n",
    "\n",
    "* El agente debería obtener una recompensa levemente negativa por no llegar al destino después de cada paso de tiempo. \"ligeramente\" negativo porque preferiríamos que nuestro agente llegara tarde en lugar de hacer movimientos equivocados tratando de llegar al destino lo más rápido posible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhFfOoFTjsOC"
   },
   "source": [
    "### 2. Espacio de Estados\n",
    "\n",
    "En RL, el agente se encuentra con un estado y luego actúa de acuerdo con el estado en el que se encuentra.\n",
    "\n",
    "El **Espacio de Estados** o *State Space* es el conjunto de todas las situaciones posibles en las que podría vivir nuestro taxi. El estado debe contener información útil que el agente necesita para realizar la acción correcta.\n",
    "\n",
    "Digamos que tenemos un área de entrenamiento para nuestro taxi inteligente donde lo estamos enseñando a transportar personas en un estacionamiento a cuatro ubicaciones diferentes (R, G, Y, B):\n",
    "\n",
    "![](https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png)\n",
    "\n",
    "Supongamos que el taxi es el único vehículo en este estacionamiento. Podemos dividir el estacionamiento en una cuadrícula de 5x5, lo que nos da 25 posibles ubicaciones de taxis. Estas 25 ubicaciones son una parte de nuestro espacio de estado. Notemos que el estado de ubicación actual de nuestro taxi es la coordenada (3, 1).\n",
    "\n",
    "También hay cuatro (4) ubicaciones donde podemos recoger y dejar un pasajero: R, G, Y, B o `[(0,0), (0,4), (4,0), (4,3)]` en las coordenadas (fila, columna). Nuestro pasajero está en la ubicación **Y** y desea ir a la ubicación **R**.\n",
    "\n",
    "Además podemos tener un (1) estado adicional de pasajero dentro del taxi. También tomamos todas las combinaciones de ubicaciones de pasajeros y ubicaciones de destino para llegar a un número total de estados para nuestro entorno de taxi; hay cuatro (4) destinos y cinco (4 + 1) ubicaciones de pasajeros.\n",
    "\n",
    "Entonces, nuestro entorno de taxi tiene 5 × 5 × 5 × 4 = 500 estados posibles totales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xH_wdCYeo6LE"
   },
   "source": [
    "### 3. Espacio de Acción\n",
    "\n",
    "El agente se encuentra con uno de los 500 estados y realiza una acción. La acción en nuestro caso puede ser moverse en una dirección o decidir recoger / dejar a un pasajero.\n",
    "\n",
    "En otras palabras, tenemos seis acciones posibles:\n",
    "1. `south`\n",
    "2. `north`\n",
    "3. `east`\n",
    "4. `west`\n",
    "5. `pickup`\n",
    "6. `dropoff`\n",
    "\n",
    "Este es el **espacio de acción**: el conjunto de todas las acciones que nuestro agente puede realizar en un estado determinado.\n",
    "\n",
    "En la ilustración anterior el taxi no puede realizar ciertas acciones en ciertos estados debido a las paredes. En el código del entorno, simplemente proporcionaremos una penalización de -1 por cada golpe de pared y el taxi no se moverá a ningún lado. Esto solo acumulará multas y hará que el taxi considere dar la vuelta al muro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vWczuvqqHMW"
   },
   "source": [
    "### Implementación en Python\n",
    "\n",
    "Afortunadamente, [OpenAI Gym](https://gym.openai.com/) tiene este entorno exacto ya construido para nosotros.\n",
    "\n",
    "Gym proporciona diferentes entornos de juego que podemos conectar a nuestro código y probar un agente. La biblioteca se encarga de la API para proporcionar toda la información que nuestro agente requeriría, como posibles acciones, puntuación y estado actual. Solo necesitamos enfocarnos en la parte del algoritmo para nuestro agente.\n",
    "\n",
    "Usaremos el entorno Gym llamado `Taxi-V3`, del que se extrajeron todos los detalles explicados anteriormente. Los objetivos, recompensas y acciones son todos iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlgEpuxmrNUH"
   },
   "source": [
    "### Utilizando la librería\n",
    "\n",
    "Necesitamos instalar el gym primero. Ejecute lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvzVprH6cG12"
   },
   "outputs": [],
   "source": [
    "!pip install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgBf3AJmro9h"
   },
   "source": [
    "Una vez instalado, podemos cargar el entorno del juego y reproducir lo que parece:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ccC3ETkcOUu"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLkJ2p9vsXoK"
   },
   "source": [
    "La interfaz principal de gym es `env`, que es la interfaz del entorno unificado. Los siguientes son los métodos de `env` que nos serán de gran ayuda:\n",
    "* `env.reset`: Restablece el entorno y devuelve un estado inicial aleatorio.\n",
    "* `env.step(action)`: Paso en el entorno por un paso de tiempo. Devuelve:\n",
    "  + **observation**: Observaciones del entorno\n",
    "  + **reward**: Si tu acción fue beneficiosa o no\n",
    "  + **done**: Indica si hemos recogido y dejado a un pasajero con éxito, también llamado _episodio_\n",
    "  + **info**: Información adicional como el rendimiento y la latencia para fines de _debugging_\n",
    "* `env.render`: Representa un frame del entorno (útil para visualizar el entorno)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mo6TcOgWuS5d"
   },
   "source": [
    "###  Volvamos a nuestro problema\n",
    "Aquí está nuestra declaración de problema reestructurada (de Gym docs):\n",
    "\n",
    "_\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\"_\n",
    "\n",
    "Entremos más en detalle en el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LhcwBvcEcn02"
   },
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LXZn4y1u_ov"
   },
   "source": [
    "El **cuadrado relleno** representa el taxi, que es amarillo sin pasajero y verde con pasajero.\n",
    "\n",
    "El **tubo (\"|\")** representa una pared que el taxi no puede cruzar.\n",
    "\n",
    "**R, G, Y, B** son las posibles ubicaciones de recogida y destino. La **letra azul** representa la ubicación actual de recogida de pasajeros y la **letra morada** es el destino actual.\n",
    "\n",
    "Como lo verifican las impresiones, tenemos un **Action Space** de tamaño 6 y un **State Space** de tamaño 500. Como ves, nuestro algoritmo RL no necesitará más información que estas dos cosas. Todo lo que necesitamos es una forma de identificar un estado de forma única asignando un número único a cada estado posible, y RL aprende a elegir un número de acción de 0 a 5 donde:\n",
    "* 0 = south\n",
    "* 1 = north\n",
    "* 2 = east\n",
    "* 3 = west\n",
    "* 4 = pickup\n",
    "* 5 = dropoff\n",
    "\n",
    "Recuerde que los 500 estados corresponden a una codificación de la ubicación del taxi, la ubicación del pasajero y la ubicación de destino.\n",
    "\n",
    "El aprendizaje por refuerzo aprenderá un mapeo de **estados** para la **acción** óptima a realizar en ese estado mediante la _exploración_, es decir, el agente explora el entorno y toma acciones basadas en recompensas definidas en el entorno.\n",
    "\n",
    "La acción óptima para cada estado es la acción que tiene **la recompensa acumulativa a largo plazo más alta**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnkUcaATwpxh"
   },
   "source": [
    "### **De vuelta a nuestra ilustración**\n",
    "De hecho, podemos tomar nuestra ilustración anterior, codificar su estado y dárselo al entorno para que se pueda reproducir en Gym. Recuerde que tenemos el taxi en la fila 3, columna 1, nuestro pasajero está en la ubicación 2 y nuestro destino es la ubicación 0. Usando el método de codificación de estado Taxi-v3, podemos hacer lo siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDOhdhLgcss6"
   },
   "outputs": [],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCDAxh2Fx-Tg"
   },
   "source": [
    "Estamos usando las coordenadas de nuestra ilustración para generar un número correspondiente a un estado entre 0 y 499, que resulta ser **328** para el estado de nuestra ilustración.\n",
    "\n",
    "Luego, podemos establecer el estado del entorno manualmente con `env.env.s` usando ese número codificado. Puede jugar con los números y verá que el taxi, el pasajero y el destino se mueven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suh-Diq4yg6W"
   },
   "source": [
    "### La tabla de recompensas\n",
    "\n",
    "Cuando se crea el entorno de Taxi, también se crea una tabla de recompensa inicial, llamada `P`. Podemos pensar en ello como una matriz que tiene el número de estados como filas y el número de acciones como columnas, es decir, una matriz de *estados* × *acciones*.\n",
    "\n",
    "Dado que cada estado está en esta matriz, podemos ver los valores de recompensa predeterminados asignados al estado de nuestra ilustración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9a12qlic3Rc"
   },
   "outputs": [],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71172xVAzNxv"
   },
   "source": [
    "Este diccionario tiene la estructura `{action: [(probability, nextstate, reward, done)]}`\n",
    "\n",
    "Algunas cosas a tener en cuenta:\n",
    "\n",
    "* El 0-5 corresponde a las acciones (south, north, east, west, pickup, dropoff) que el taxi puede realizar en nuestro estado actual en la ilustración.\n",
    "* En este env, la `probability` siempre es 1.0.\n",
    "* El `nextstate` es el estado en el que estaríamos si tomamos la acción en este índice del dict\n",
    "* Todas las acciones de movimiento tienen una recompensa de -1 y las acciones de recoger / dejar tienen una recompensa de -10 en este estado en particular. Si estamos en un estado en el que el taxi tiene un pasajero y está en la parte superior del destino correcto, veríamos una recompensa de 20 en la acción de dropoff (5)\n",
    "* `done` se utiliza para indicarnos cuándo hemos dejado a un pasajero en el lugar correcto. Cada dejada de pasajeros exitoso es el final de un **episodio**.\n",
    "\n",
    "Ten en cuenta que si nuestro agente eligiera explorar la acción tres (3) en este estado, estaría yendo hacia el Oeste contra una pared. El código fuente ha hecho imposible mover el taxi a través de una pared, por lo que si el taxi elige esa acción, seguirá acumulando -1 penalizaciones, lo que afecta la **recompensa a largo plazo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfJ6AS_S103I"
   },
   "source": [
    "### Resolviendo el entorno sin RL\n",
    "Veamos qué pasaría si intentamos utilizar la fuerza bruta para resolver el problema sin RL.\n",
    "\n",
    "Dado que tenemos nuestra tabla P para las recompensas predeterminadas en cada estado, podemos intentar que nuestro taxi navegue solo con eso.\n",
    "\n",
    "Crearemos un ciclo infinito que se ejecutará hasta que un pasajero llegue a un destino (un **episodio**), o en otras palabras, cuando la recompensa recibida sea 20. El método `env.action_space.sample ()` selecciona automáticamente una acción aleatoria del conjunto de todos posibles acciones.\n",
    "\n",
    "Veamos qué pasa (**Spoiler**: se va a demorar demasiado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqBfrUOedCZh"
   },
   "outputs": [],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snEbr18gdLAt"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, X in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(X['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {X['state']}\")\n",
    "        print(f\"Action: {X['action']}\")\n",
    "        print(f\"Reward: {X['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZdlgZ2V3GpL"
   },
   "source": [
    "No está bien. Nuestro agente toma cientos (o miles) de pasos de tiempo y realiza muchas entregas incorrectas para entregar solo un pasajero al destino correcto.\n",
    "\n",
    "Esto se debe a que no estamos _aprendiendo_ de experiencias pasadas. Podemos ejecutar esto una y otra vez, y nunca se optimizará. El agente no tiene memoria de qué acción fue la mejor para cada estado, que es exactamente lo que hará por nosotros el Aprendizaje por refuerzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyGJ52yR3w_3"
   },
   "source": [
    "### Utilizando Reinforcement Learning\n",
    "\n",
    "Vamos a utilizar un algoritmo RL simple llamado _Q-learning_ que le dará a nuestro agente algo de memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atzDfXD-4GxR"
   },
   "source": [
    "### Intro a Q-learning\n",
    "\n",
    "Esencialmente, Q-learning permite al agente usar las recompensas del entorno para aprender, con el tiempo, la mejor acción a tomar en un estado determinado.\n",
    "\n",
    "En nuestro entorno de Taxi, tenemos la tabla de recompensas, `P`, de la que el agente aprenderá. Lo hace buscando recibir una recompensa por realizar una acción en el estado actual y luego actualizar un _Q-value_ para recordar si esa acción fue beneficiosa.\n",
    "\n",
    "Los valores almacenados en la Q-table se denominan _Q-values_ y se asignan a una combinación `(state, action)`.\n",
    "\n",
    "Un Q-value para una combinación de acción de estado particular es representativo de la \"calidad\" de una acción tomada desde ese estado. Mejores Q-values implican mejores posibilidades de obtener mayores recompensas.\n",
    "\n",
    "Por ejemplo, si el taxi se enfrenta a un estado que incluye a un pasajero en su ubicación actual, es muy probable que el Q-value de `pickup` sea más alto en comparación con otras acciones, como `dropoff` o `north`.\n",
    "\n",
    "Los Q-values se inicializan a un valor arbitrario y, a medida que el agente se expone al entorno y recibe diferentes recompensas al ejecutar diferentes acciones, los Q-values se actualizan utilizando la ecuación:\n",
    "\n",
    "$$Q(state,action)\\leftarrow (1-\\alpha)Q(state,action) + \\alpha\\Big(reward + \\gamma \\max_{a}Q(next\\mbox{ }state,all\\mbox{ }actions)\\Big)$$\n",
    "\n",
    "Donde:\n",
    "* $\\alpha$ (alfa) es la tasa de aprendizaje $(0<\\alpha\\leq 1)$ - Al igual que en los entornos de aprendizaje supervisado, $\\alpha$ es el grado en que nuestros Q-values se actualizan en cada iteración.\n",
    "* $\\gamma$ (gamma) es el factor de descuento $(0\\leq \\gamma\\leq 1)$: determina cuánta importancia queremos dar a las recompensas futuras. Un valor alto para el factor de descuento (cercano a **1**) captura la recompensa efectiva a largo plazo, mientras que un factor de descuento de **0** hace que nuestro agente considere solo la recompensa inmediata, lo que lo hace glotón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4mqUFtL9CEJ"
   },
   "source": [
    "### ¿Qué nos dice esto?\n",
    "\n",
    "Estamos asignando $(\\leftarrow)$, o actualizando, el Q-value del _estado_ actual y la _acción_ del agente tomando primero un peso $(1-\\alpha)$ del antiguo Q-value y luego sumando el valor aprendido. El valor aprendido es una combinación de la recompensa por realizar la acción actual en el estado actual y la recompensa máxima descontada del siguiente estado en el que estaremos una vez que realicemos la acción actual.\n",
    "\n",
    "Básicamente, estamos aprendiendo la acción adecuada a tomar en el estado actual al observar la recompensa por el combo estado / acción actual y las recompensas máximas para el siguiente estado. Esto eventualmente hará que nuestro taxi considere la ruta con las mejores recompensas juntas.\n",
    "\n",
    "El Q-value de un par estado-acción es la suma de la recompensa instantánea y la recompensa futura descontada (del estado resultante). La forma en que almacenamos los Q-values para cada estado y acción es a través de una **Q-table**.\n",
    "\n",
    "### Q-Table\n",
    "\n",
    "La Q-table es una matriz donde tenemos una fila para cada estado (500) y una columna para cada acción (6). Primero se inicializa a 0 y luego los valores se actualizan después del entrenamiento. Ten en cuenta que la Q-table tiene las mismas dimensiones que la tabla de recompensas, pero tiene un propósito completamente diferente.\n",
    "\n",
    "![Los valores de Q-Table se inicializan a cero y luego se actualizan durante el entrenamiento a valores que optimizan el recorrido del agente por el entorno para obtener las máximas recompensas.](https://storage.googleapis.com/lds-media/images/q-matrix-initialized-to-learned_gQq0BFs.width-1200.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55mvMVvu_It_"
   },
   "source": [
    "### Resumiendo el proceso Q-learning\n",
    "* Inicialice la Q-table con ceros.\n",
    "* Empiece a explorar acciones: para cada estado, seleccione cualquiera de las posibles acciones para el estado actual (S).\n",
    "* Viaja al siguiente estado (S') como resultado de esa acción (a).\n",
    "* Para todas las acciones posibles del estado (S'), seleccione la que tenga el Q-value más alto.\n",
    "* Actualice los valores de la Q-table usando la ecuación.\n",
    "* Establezca el siguiente estado como el estado actual.\n",
    "* Si se alcanza el estado objetivo, finalice y repita el proceso.\n",
    "\n",
    "### Explotación de valores aprendidos\n",
    "\n",
    "Después de suficiente exploración aleatoria de acciones, los Q-values tienden a converger sirviendo a nuestro agente como una función de valor de acción que puede explotar para elegir la acción más óptima de un estado dado.\n",
    "\n",
    "Existe una compensación entre la exploración (elegir una acción aleatoria) y la explotación (elegir acciones basadas en los Q-values ya aprendidos). Queremos evitar que la acción siga siempre la misma ruta y posiblemente se sobreajuste, así que vamos a introducir otro parámetro llamado $\\epsilon$ \"épsilon\" para atender esto durante el entrenamiento.\n",
    "\n",
    "En lugar de simplemente seleccionar la acción de Q-value mejor aprendida, a veces preferimos explorar más el espacio de acción. Un valor épsilon más bajo da como resultado episodios con más penalizaciones (en promedio), lo cual es obvio porque estamos explorando y tomando decisiones al azar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zY3t2_91BRHT"
   },
   "source": [
    "### Implementando Q-learning en Python\n",
    "\n",
    "Primero, inicializaremos la Q-table en una matriz de ceros de 500 × 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MW1xNK-eGEe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvBqhmktBm46"
   },
   "source": [
    "Ahora podemos crear el algoritmo de entrenamiento que actualizará esta Q-table a medida que el agente explora el entorno durante miles de episodios.\n",
    "\n",
    "En la primera parte de `while not done` decidimos si elegir una acción aleatoria o explotar los Q-values ya calculados. Esto se hace simplemente usando el valor `epsilon` y comparándolo con la función `random.uniform (0, 1)`, que devuelve un número arbitrario entre 0 y 1.\n",
    "\n",
    "Ejecutamos la acción elegida en el entorno para obtener el `next_state` y la `reward` por realizar la acción. Después de eso, calculamos el Q-value máximo para las acciones correspondientes al next_state, y con eso, podemos actualizar fácilmente nuestro Q-value al `new_q_value`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UQf-pzLeHyk"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuFYug6eDEkt"
   },
   "source": [
    "Ahora que la Q-table se ha establecido en más de 100.000 episodios, veamos cuáles son los Q-values en el estado de nuestra ilustración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xHNDcaienxZ"
   },
   "outputs": [],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRXx-0ydDQmW"
   },
   "source": [
    "El Q-value máximo es \"north\", por lo que parece que Q-learning ha aprendido efectivamente la mejor acción a realizar en el estado de nuestra ilustración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUSqsAXfDo68"
   },
   "source": [
    "### Evaluando a un agente\n",
    "Evaluemos el desempeño de nuestro agente. No necesitamos explorar más acciones, por lo que ahora la siguiente acción siempre se selecciona utilizando el mejor Q-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sq-1eQyqeuY2"
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmbwocYpEFhi"
   },
   "source": [
    "Podemos ver en la evaluación que el desempeño del agente mejoró significativamente y no incurrió en penalizaciones, lo que significa que realizó las acciones correctas de pickup/dropoff con 100 pasajeros diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tfhS3-k6E4Gf"
   },
   "source": [
    "### Comparando nuestro agente de Q-learning con sin RL\n",
    "Con Q-learning, el agente comete errores inicialmente durante la exploración, pero una vez que ha explorado lo suficiente (visto la mayoría de los estados), puede actuar sabiamente maximizando las recompensas haciendo movimientos inteligentes. Veamos cuánto mejor es nuestra solución de Q-learning en comparación con el agente que realiza movimientos aleatorios.\n",
    "\n",
    "Evaluamos a nuestros agentes de acuerdo con las siguientes métricas,\n",
    "\n",
    "* **El promedio de penalizaciones por episodio**: cuanto menor sea el número, mejor será el rendimiento de nuestro agente. Idealmente, nos gustaría que esta métrica fuera cero o muy cercana a cero.\n",
    "\n",
    "* **El promedio de pasos de tiempo por viaje**: también queremos un número pequeño de pasos de tiempo por episodio, ya que queremos que nuestro agente dé pasos mínimos (es decir, el camino más corto) para llegar al destino.\n",
    "\n",
    "* **Recompensas promedio por movimiento**: cuanto mayor sea la recompensa, significa que el agente está haciendo lo correcto. Es por eso que decidir las recompensas es una parte crucial del aprendizaje por refuerzo. En nuestro caso, dado que tanto los tiempos como las penalizaciones se recompensan negativamente, una recompensa promedio más alta significaría que el agente llega al destino lo más rápido posible con la menor cantidad de penalizaciones\n",
    "\n",
    "</li></ul></div></div></section><section><div class=\"table-block table-style-filled-headers \"><table><thead><tr><th>Measure</th><th>Random agent's performance</th><th>Q-learning agent's performance</th></tr></thead><tbody><tr><td>Average rewards per move</td><td>-3.9012092102214075</td><td>0.6962843295638126</td></tr><tr><td>Average number of penalties per episode</td><td>920.45</td><td>0.0</td></tr><tr><td>Average number of timesteps per trip</td><td>2848.14</td><td>12.38</td></tr></tbody></table></div></section><section><div class=\"paragraph read-friendly\"><div class=\"rich-text\"><p>\n",
    "\n",
    "Estas métricas se calcularon en más de 100 episodios. Y como muestran los resultados, ¡nuestro agente de Q-learning lo logró!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9iuATL7HsZg"
   },
   "source": [
    "### Hiperparámetros y optimizaciones\n",
    "\n",
    "Los valores de `alpha`,` gamma` y `epsilon` se basaron principalmente en la intuición y algunos \"prueba y error\", pero hay mejores formas de obtener buenos valores.\n",
    "\n",
    "Idealmente, los tres deberían disminuir con el tiempo porque a medida que el agente continúa aprendiendo, en realidad construye antecedentes más resistentes;\n",
    "\n",
    "$\\alpha$: (la tasa de aprendizaje) debería disminuir a medida que continúa adquiriendo una base de conocimientos cada vez más grande.\n",
    "\n",
    "$\\gamma$: a medida que se acerca más y más a la fecha límite, su preferencia por la recompensa a corto plazo debería aumentar, ya que no estará el tiempo suficiente para obtener la recompensa a largo plazo, lo que significa que su gamma debería disminuir.\n",
    "\n",
    "$\\epsilon$: a medida que desarrollamos nuestra estrategia, tenemos menos necesidad de exploración y más explotación para obtener más utilidad de nuestra política, por lo que a medida que aumentan los ensayos, épsilon debería disminuir.\n",
    "\n",
    "Ajustando los hiperparámetros\n",
    "\n",
    "Una forma sencilla de generar mediante programación el mejor conjunto de valores del hiperparámetro es crear una función de búsqueda completa (similar a grid search) que seleccione los parámetros que darían como resultado la mejor relación `reward/time_steps`. El motivo de `reward/time_steps` es que queremos elegir parámetros que nos permitan obtener la máxima recompensa lo más rápido posible. También es posible que deseemos realizar un seguimiento del número de penalizaciones correspondientes a la combinación de valores de hiperparámetro porque esto también puede ser un factor decisivo (no queremos que nuestro agente inteligente viole las reglas a costa de llegar más rápido). Una forma más elegante de obtener la combinación correcta de valores de hiperparámetros sería utilizar _algoritmos genéticos_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDz08lzyJbkt"
   },
   "source": [
    "### Palabras al cierre\n",
    "Q-learning es uno de los algoritmos de RL más fáciles. Sin embargo, el problema con Q-learning es que, una vez que el número de estados en el entorno es muy alto, se vuelve difícil implementarlos con la Q-table ya que el tamaño sería muy, muy grande. Las técnicas de vanguardia utilizan redes neuronales profundas en lugar de Q-table (Deep Reinforcement Learning). La red neuronal recibe información de estado y acciones en la capa de entrada y aprende a generar la acción correcta a lo largo del tiempo. Las técnicas de aprendizaje profundo (como las redes neuronales convolucionales) también se utilizan para interpretar los píxeles en la pantalla y extraer información del juego (como puntuaciones), y luego dejar que el agente controle el juego.\n",
    "\n",
    "Hemos hablado mucho sobre RL y los juegos. Pero el RL no se limita solo a los juegos. Se utiliza para administrar carteras de valores y finanzas, para fabricar robots humanoides, para la fabricación y la gestión de inventarios, para desarrollar agentes de IA generales, que son agentes que pueden realizar varias cosas con un solo algoritmo, como el mismo agente que juega varios juegos de Atari. Open AI también tiene una plataforma llamada _universe_ para medir y entrenar la inteligencia general de una IA en miles de juegos, sitios web y otras aplicaciones generales."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "03_RL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_metadata": {
   "author": "Joaquin Vanschoren",
   "title": "Introduction"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
